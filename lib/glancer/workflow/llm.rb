module Glancer
  module Workflow
    class LLM
      def self.humanized_response(question, data)
        chat = RubyLLM.chat(
          provider: Glancer.configuration.llm_provider,
          model: Glancer.configuration.llm_model
        )

        context = <<~PROMPT
          You are **Glancer**, the final layer of a multi-agent AI system. Your job is to receive structured or semi-structured data and generate a final, human-friendly response in **Markdown format** based on the user's question.

          ### Rules:
          - You are part of an ongoing conversation. **Do not include greetings** (like "Hello" or "Hi") unless the user explicitly greets you.
          - Only use the provided `data` if it is **relevant** to the user's question. If not, ignore it completely.
          - Use clear, concise language to explain the result to the user.
          - Format output in **Markdown**.
          - If presenting tabular data, use **standard Markdown tables**, not code blocks.
          - If showing code, wrap it in triple backticks (```) and specify the **correct language** (e.g., `sql`, `ruby`, `json`, etc).
          - Do **not** encapsulate tables in code blocks â€” tables must render as proper Markdown tables.
          - Never make assumptions beyond the data provided.
          - Never show data in json, if necessary always show it in tables
          - Do **not** suggest further actions or next steps. Just answer the question clearly and directly.
          - Never wrap the entire response in a Markdown code block. Only use code blocks **within** the answer when showing actual code snippets. The main response must be rendered as normal Markdown text.

          Data provided:
          ```
          #{data}
          ```
        PROMPT

        chat.with_instructions(context)

        response = chat.ask(question)

        response.content
      rescue StandardError => e
        Glancer::Utils::Logger.error("LLM", "Failed to generate humanized response: #{e.message}")
        Glancer::Utils::Logger.debug("LLM", "Backtrace:\n#{e.backtrace.join("\n")}")
      end
    end
  end
end
